{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Neural machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5156,
     "status": "ok",
     "timestamp": 1619789089261,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "CNvjhDyAKk3U",
    "outputId": "3166c525-fc56-4fd1-92b2-2f1ac4ac18a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-28 16:24:31--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Распознаётся www.manythings.org (www.manythings.org)… 173.254.30.110\n",
      "Подключение к www.manythings.org (www.manythings.org)|173.254.30.110|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 7757635 (7.4M) [application/zip]\n",
      "Сохранение в: «fra-eng.zip.1»\n",
      "\n",
      "fra-eng.zip.1       100%[===================>]   7.40M  3.41MB/s    за 2.2s    \n",
      "\n",
      "2023-08-28 16:24:34 (3.41 MB/s) - «fra-eng.zip.1» сохранён [7757635/7757635]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1813,
     "status": "ok",
     "timestamp": 1619789098992,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "83bg17Lr-7XK",
    "outputId": "91ec0134-b351-4d0a-c8ac-215affefed97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: fra-eng: File exists\n",
      "Archive:  fra-eng.zip\n",
      "replace fra-eng/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir fra-eng\n",
    "!unzip fra-eng.zip -d fra-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1619789101916,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "7o5L92efMMhf",
    "outputId": "6ac53232-05f5-449b-8cfc-e0dda693d57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: -lah: No such file or directory\r\n",
      "fra-eng/:\r\n",
      "_about.txt fra.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls fra-eng/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_file = \"fra-eng/fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1619789112313,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "yV9lZXQXNbnH",
    "outputId": "2f509965-ecec-4988-cffa-09ab9f954f13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENG, RUS]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12954,
     "status": "ok",
     "timestamp": 1619808083270,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "cTbSbBz55QtF",
    "outputId": "5a5b0b4e-da16-4483-8e56-5e07fd79f0bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go . <end>\n",
      "<start> va ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, fr = create_dataset(path_to_file, None)\n",
    "print(en[0])\n",
    "print(fr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1074,
     "status": "ok",
     "timestamp": 1619789980135,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "C8j9g9AnIeZV",
    "outputId": "510da3ec-5a6d-49b9-a0da-29c693f1aec5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227815, 227815)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 100000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1619790003654,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "4QILQkOs3jFG",
    "outputId": "ddc672e5-2231-4985-b550-2dbddd4468b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 80000 20000 20000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1619790005935,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "VXukARTDd7MT",
    "outputId": "a3606064-f48d-4e81-94d5-9c30d5e90ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4279 ----> voyagez\n",
      "6 ----> vous\n",
      "315 ----> seule\n",
      "5 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "19 ----> are\n",
      "5 ----> you\n",
      "1236 ----> traveling\n",
      "136 ----> alone\n",
      "6 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 300\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1619790037553,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "5f42bae2-eefc-492f-f5c5-ad5215d549cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 19]), TensorShape([64, 12]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27335,
     "status": "ok",
     "timestamp": 1619790133885,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "60gSVh05Jl6l",
    "outputId": "4eaa568c-966c-4a9b-895d-4d5e22d20e3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 955,
     "status": "ok",
     "timestamp": 1619791577723,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "XKcypC0AGeLR",
    "outputId": "6f51f5ef-6d92-4933-d202-e2d512cbba37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 8799])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1619791547930,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "6y0HF-zMF_vp",
    "outputId": "69a0893f-61c6-40c5-bce6-5911c3c4355b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10943081,
     "status": "ok",
     "timestamp": 1619803077637,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "ddefjBMa3jF0",
    "outputId": "b54245db-e06a-46b0-edc0-fab6afaba17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6005\n",
      "Epoch 1 Batch 100 Loss 2.1720\n",
      "Epoch 1 Batch 200 Loss 1.8914\n",
      "Epoch 1 Batch 300 Loss 1.8370\n",
      "Epoch 1 Batch 400 Loss 1.7023\n",
      "Epoch 1 Batch 500 Loss 1.6483\n",
      "Epoch 1 Batch 600 Loss 1.6539\n",
      "Epoch 1 Batch 700 Loss 1.8174\n",
      "Epoch 1 Batch 800 Loss 1.6562\n",
      "Epoch 1 Batch 900 Loss 1.4531\n",
      "Epoch 1 Batch 1000 Loss 1.2654\n",
      "Epoch 1 Batch 1100 Loss 1.3141\n",
      "Epoch 1 Batch 1200 Loss 1.2204\n",
      "Epoch 1 Loss 1.6605\n",
      "Time taken for 1 epoch 446.1971559524536 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.1915\n",
      "Epoch 2 Batch 100 Loss 1.1043\n",
      "Epoch 2 Batch 200 Loss 1.2825\n",
      "Epoch 2 Batch 300 Loss 1.0896\n",
      "Epoch 2 Batch 400 Loss 1.1058\n",
      "Epoch 2 Batch 500 Loss 1.0992\n",
      "Epoch 2 Batch 600 Loss 1.0320\n",
      "Epoch 2 Batch 700 Loss 0.9814\n",
      "Epoch 2 Batch 800 Loss 0.8737\n",
      "Epoch 2 Batch 900 Loss 0.8621\n",
      "Epoch 2 Batch 1000 Loss 1.0349\n",
      "Epoch 2 Batch 1100 Loss 0.8576\n",
      "Epoch 2 Batch 1200 Loss 0.8188\n",
      "Epoch 2 Loss 0.9754\n",
      "Time taken for 1 epoch 443.89236092567444 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6267\n",
      "Epoch 3 Batch 100 Loss 0.6422\n",
      "Epoch 3 Batch 200 Loss 0.6352\n",
      "Epoch 3 Batch 300 Loss 0.6573\n",
      "Epoch 3 Batch 400 Loss 0.6276\n",
      "Epoch 3 Batch 500 Loss 0.5853\n",
      "Epoch 3 Batch 600 Loss 0.6175\n",
      "Epoch 3 Batch 700 Loss 0.5689\n",
      "Epoch 3 Batch 800 Loss 0.5358\n",
      "Epoch 3 Batch 900 Loss 0.6622\n",
      "Epoch 3 Batch 1000 Loss 0.5057\n",
      "Epoch 3 Batch 1100 Loss 0.5518\n",
      "Epoch 3 Batch 1200 Loss 0.6016\n",
      "Epoch 3 Loss 0.6097\n",
      "Time taken for 1 epoch 437.8072335720062 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3852\n",
      "Epoch 4 Batch 100 Loss 0.3397\n",
      "Epoch 4 Batch 200 Loss 0.4263\n",
      "Epoch 4 Batch 300 Loss 0.4471\n",
      "Epoch 4 Batch 400 Loss 0.4323\n",
      "Epoch 4 Batch 500 Loss 0.3105\n",
      "Epoch 4 Batch 600 Loss 0.3850\n",
      "Epoch 4 Batch 700 Loss 0.4083\n",
      "Epoch 4 Batch 800 Loss 0.3597\n",
      "Epoch 4 Batch 900 Loss 0.3658\n",
      "Epoch 4 Batch 1000 Loss 0.3573\n",
      "Epoch 4 Batch 1100 Loss 0.4356\n",
      "Epoch 4 Batch 1200 Loss 0.4284\n",
      "Epoch 4 Loss 0.3869\n",
      "Time taken for 1 epoch 438.67715406417847 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2427\n",
      "Epoch 5 Batch 100 Loss 0.2671\n",
      "Epoch 5 Batch 200 Loss 0.2296\n",
      "Epoch 5 Batch 300 Loss 0.2534\n",
      "Epoch 5 Batch 400 Loss 0.2824\n",
      "Epoch 5 Batch 500 Loss 0.2931\n",
      "Epoch 5 Batch 600 Loss 0.2473\n",
      "Epoch 5 Batch 700 Loss 0.2555\n",
      "Epoch 5 Batch 800 Loss 0.2619\n",
      "Epoch 5 Batch 900 Loss 0.3020\n",
      "Epoch 5 Batch 1000 Loss 0.2550\n",
      "Epoch 5 Batch 1100 Loss 0.2242\n",
      "Epoch 5 Batch 1200 Loss 0.2797\n",
      "Epoch 5 Loss 0.2609\n",
      "Time taken for 1 epoch 439.7043800354004 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1976\n",
      "Epoch 6 Batch 100 Loss 0.1387\n",
      "Epoch 6 Batch 200 Loss 0.1778\n",
      "Epoch 6 Batch 300 Loss 0.1584\n",
      "Epoch 6 Batch 400 Loss 0.1847\n",
      "Epoch 6 Batch 500 Loss 0.1742\n",
      "Epoch 6 Batch 600 Loss 0.2049\n",
      "Epoch 6 Batch 700 Loss 0.2247\n",
      "Epoch 6 Batch 800 Loss 0.2066\n",
      "Epoch 6 Batch 900 Loss 0.2174\n",
      "Epoch 6 Batch 1000 Loss 0.2261\n",
      "Epoch 6 Batch 1100 Loss 0.1790\n",
      "Epoch 6 Batch 1200 Loss 0.2143\n",
      "Epoch 6 Loss 0.1904\n",
      "Time taken for 1 epoch 439.2271409034729 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1366\n",
      "Epoch 7 Batch 100 Loss 0.1336\n",
      "Epoch 7 Batch 200 Loss 0.1483\n",
      "Epoch 7 Batch 300 Loss 0.1311\n",
      "Epoch 7 Batch 400 Loss 0.1530\n",
      "Epoch 7 Batch 500 Loss 0.1715\n",
      "Epoch 7 Batch 600 Loss 0.1877\n",
      "Epoch 7 Batch 700 Loss 0.1403\n",
      "Epoch 7 Batch 800 Loss 0.1518\n",
      "Epoch 7 Batch 900 Loss 0.1393\n",
      "Epoch 7 Batch 1000 Loss 0.1591\n",
      "Epoch 7 Batch 1100 Loss 0.1806\n",
      "Epoch 7 Batch 1200 Loss 0.1362\n",
      "Epoch 7 Loss 0.1515\n",
      "Time taken for 1 epoch 437.3923101425171 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1275\n",
      "Epoch 8 Batch 100 Loss 0.1177\n",
      "Epoch 8 Batch 200 Loss 0.1223\n",
      "Epoch 8 Batch 300 Loss 0.1262\n",
      "Epoch 8 Batch 400 Loss 0.1043\n",
      "Epoch 8 Batch 500 Loss 0.0866\n",
      "Epoch 8 Batch 600 Loss 0.1343\n",
      "Epoch 8 Batch 700 Loss 0.1662\n",
      "Epoch 8 Batch 800 Loss 0.1280\n",
      "Epoch 8 Batch 900 Loss 0.1168\n",
      "Epoch 8 Batch 1000 Loss 0.1301\n",
      "Epoch 8 Batch 1100 Loss 0.1472\n",
      "Epoch 8 Batch 1200 Loss 0.1227\n",
      "Epoch 8 Loss 0.1293\n",
      "Time taken for 1 epoch 437.76856803894043 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0895\n",
      "Epoch 9 Batch 100 Loss 0.0815\n",
      "Epoch 9 Batch 200 Loss 0.1009\n",
      "Epoch 9 Batch 300 Loss 0.1314\n",
      "Epoch 9 Batch 400 Loss 0.1212\n",
      "Epoch 9 Batch 500 Loss 0.1459\n",
      "Epoch 9 Batch 600 Loss 0.1412\n",
      "Epoch 9 Batch 700 Loss 0.1109\n",
      "Epoch 9 Batch 800 Loss 0.0991\n",
      "Epoch 9 Batch 900 Loss 0.0952\n",
      "Epoch 9 Batch 1000 Loss 0.1077\n",
      "Epoch 9 Batch 1100 Loss 0.1377\n",
      "Epoch 9 Batch 1200 Loss 0.1271\n",
      "Epoch 9 Loss 0.1146\n",
      "Time taken for 1 epoch 440.3663229942322 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0729\n",
      "Epoch 10 Batch 100 Loss 0.1126\n",
      "Epoch 10 Batch 200 Loss 0.0640\n",
      "Epoch 10 Batch 300 Loss 0.0759\n",
      "Epoch 10 Batch 400 Loss 0.0769\n",
      "Epoch 10 Batch 500 Loss 0.0892\n",
      "Epoch 10 Batch 600 Loss 0.1006\n",
      "Epoch 10 Batch 700 Loss 0.1586\n",
      "Epoch 10 Batch 800 Loss 0.1025\n",
      "Epoch 10 Batch 900 Loss 0.0980\n",
      "Epoch 10 Batch 1000 Loss 0.1162\n",
      "Epoch 10 Batch 1100 Loss 0.1223\n",
      "Epoch 10 Batch 1200 Loss 0.1239\n",
      "Epoch 10 Loss 0.1055\n",
      "Time taken for 1 epoch 439.328604221344 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0741\n",
      "Epoch 11 Batch 100 Loss 0.0656\n",
      "Epoch 11 Batch 200 Loss 0.0573\n",
      "Epoch 11 Batch 300 Loss 0.0962\n",
      "Epoch 11 Batch 400 Loss 0.1014\n",
      "Epoch 11 Batch 500 Loss 0.0638\n",
      "Epoch 11 Batch 600 Loss 0.0856\n",
      "Epoch 11 Batch 700 Loss 0.0970\n",
      "Epoch 11 Batch 800 Loss 0.1109\n",
      "Epoch 11 Batch 900 Loss 0.1137\n",
      "Epoch 11 Batch 1000 Loss 0.0697\n",
      "Epoch 11 Batch 1100 Loss 0.1129\n",
      "Epoch 11 Batch 1200 Loss 0.1381\n",
      "Epoch 11 Loss 0.0986\n",
      "Time taken for 1 epoch 438.30559372901917 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0857\n",
      "Epoch 12 Batch 100 Loss 0.1250\n",
      "Epoch 12 Batch 200 Loss 0.0614\n",
      "Epoch 12 Batch 300 Loss 0.1055\n",
      "Epoch 12 Batch 400 Loss 0.1052\n",
      "Epoch 12 Batch 500 Loss 0.0897\n",
      "Epoch 12 Batch 600 Loss 0.0784\n",
      "Epoch 12 Batch 700 Loss 0.0979\n",
      "Epoch 12 Batch 800 Loss 0.0957\n",
      "Epoch 12 Batch 900 Loss 0.0882\n",
      "Epoch 12 Batch 1000 Loss 0.1045\n",
      "Epoch 12 Batch 1100 Loss 0.0962\n",
      "Epoch 12 Batch 1200 Loss 0.1145\n",
      "Epoch 12 Loss 0.0938\n",
      "Time taken for 1 epoch 436.92461681365967 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0752\n",
      "Epoch 13 Batch 100 Loss 0.0715\n",
      "Epoch 13 Batch 200 Loss 0.0876\n",
      "Epoch 13 Batch 300 Loss 0.0585\n",
      "Epoch 13 Batch 400 Loss 0.0668\n",
      "Epoch 13 Batch 500 Loss 0.0919\n",
      "Epoch 13 Batch 600 Loss 0.1199\n",
      "Epoch 13 Batch 700 Loss 0.0957\n",
      "Epoch 13 Batch 800 Loss 0.1046\n",
      "Epoch 13 Batch 900 Loss 0.0936\n",
      "Epoch 13 Batch 1000 Loss 0.0966\n",
      "Epoch 13 Batch 1100 Loss 0.0943\n",
      "Epoch 13 Batch 1200 Loss 0.1181\n",
      "Epoch 13 Loss 0.0894\n",
      "Time taken for 1 epoch 436.8003931045532 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0398\n",
      "Epoch 14 Batch 100 Loss 0.0645\n",
      "Epoch 14 Batch 200 Loss 0.0609\n",
      "Epoch 14 Batch 300 Loss 0.1117\n",
      "Epoch 14 Batch 400 Loss 0.0918\n",
      "Epoch 14 Batch 500 Loss 0.0702\n",
      "Epoch 14 Batch 600 Loss 0.0756\n",
      "Epoch 14 Batch 700 Loss 0.1056\n",
      "Epoch 14 Batch 800 Loss 0.0614\n",
      "Epoch 14 Batch 900 Loss 0.0779\n",
      "Epoch 14 Batch 1000 Loss 0.1040\n",
      "Epoch 14 Batch 1100 Loss 0.0932\n",
      "Epoch 14 Batch 1200 Loss 0.1033\n",
      "Epoch 14 Loss 0.0866\n",
      "Time taken for 1 epoch 435.3800597190857 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0636\n",
      "Epoch 15 Batch 100 Loss 0.0968\n",
      "Epoch 15 Batch 200 Loss 0.1031\n",
      "Epoch 15 Batch 300 Loss 0.0721\n",
      "Epoch 15 Batch 400 Loss 0.0800\n",
      "Epoch 15 Batch 500 Loss 0.0824\n",
      "Epoch 15 Batch 600 Loss 0.0916\n",
      "Epoch 15 Batch 700 Loss 0.0780\n",
      "Epoch 15 Batch 800 Loss 0.0847\n",
      "Epoch 15 Batch 900 Loss 0.0973\n",
      "Epoch 15 Batch 1000 Loss 0.0731\n",
      "Epoch 15 Batch 1100 Loss 0.1016\n",
      "Epoch 15 Batch 1200 Loss 0.1057\n",
      "Epoch 15 Loss 0.0842\n",
      "Time taken for 1 epoch 435.2100999355316 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0752\n",
      "Epoch 16 Batch 100 Loss 0.0430\n",
      "Epoch 16 Batch 200 Loss 0.1207\n",
      "Epoch 16 Batch 300 Loss 0.0794\n",
      "Epoch 16 Batch 400 Loss 0.1105\n",
      "Epoch 16 Batch 500 Loss 0.1281\n",
      "Epoch 16 Batch 600 Loss 0.1351\n",
      "Epoch 16 Batch 700 Loss 0.0504\n",
      "Epoch 16 Batch 800 Loss 0.1117\n",
      "Epoch 16 Batch 900 Loss 0.0739\n",
      "Epoch 16 Batch 1000 Loss 0.0545\n",
      "Epoch 16 Batch 1100 Loss 0.1123\n",
      "Epoch 16 Batch 1200 Loss 0.0962\n",
      "Epoch 16 Loss 0.0813\n",
      "Time taken for 1 epoch 438.7301571369171 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.1016\n",
      "Epoch 17 Batch 100 Loss 0.0684\n",
      "Epoch 17 Batch 200 Loss 0.0847\n",
      "Epoch 17 Batch 300 Loss 0.0614\n",
      "Epoch 17 Batch 400 Loss 0.0773\n",
      "Epoch 17 Batch 500 Loss 0.0881\n",
      "Epoch 17 Batch 600 Loss 0.0987\n",
      "Epoch 17 Batch 700 Loss 0.0922\n",
      "Epoch 17 Batch 800 Loss 0.0765\n",
      "Epoch 17 Batch 900 Loss 0.0781\n",
      "Epoch 17 Batch 1000 Loss 0.0857\n",
      "Epoch 17 Batch 1100 Loss 0.1199\n",
      "Epoch 17 Batch 1200 Loss 0.0755\n",
      "Epoch 17 Loss 0.0799\n",
      "Time taken for 1 epoch 436.6619563102722 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0478\n",
      "Epoch 18 Batch 100 Loss 0.0778\n",
      "Epoch 18 Batch 200 Loss 0.0352\n",
      "Epoch 18 Batch 300 Loss 0.0691\n",
      "Epoch 18 Batch 400 Loss 0.1330\n",
      "Epoch 18 Batch 500 Loss 0.0678\n",
      "Epoch 18 Batch 600 Loss 0.0466\n",
      "Epoch 18 Batch 700 Loss 0.0829\n",
      "Epoch 18 Batch 800 Loss 0.0912\n",
      "Epoch 18 Batch 900 Loss 0.0954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 1000 Loss 0.0997\n",
      "Epoch 18 Batch 1100 Loss 0.0749\n",
      "Epoch 18 Batch 1200 Loss 0.1389\n",
      "Epoch 18 Loss 0.0785\n",
      "Time taken for 1 epoch 438.77354979515076 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0503\n",
      "Epoch 19 Batch 100 Loss 0.0660\n",
      "Epoch 19 Batch 200 Loss 0.0541\n",
      "Epoch 19 Batch 300 Loss 0.0456\n",
      "Epoch 19 Batch 400 Loss 0.0906\n",
      "Epoch 19 Batch 500 Loss 0.0556\n",
      "Epoch 19 Batch 600 Loss 0.0772\n",
      "Epoch 19 Batch 700 Loss 0.0981\n",
      "Epoch 19 Batch 800 Loss 0.0675\n",
      "Epoch 19 Batch 900 Loss 0.0839\n",
      "Epoch 19 Batch 1000 Loss 0.1079\n",
      "Epoch 19 Batch 1100 Loss 0.0961\n",
      "Epoch 19 Batch 1200 Loss 0.0817\n",
      "Epoch 19 Loss 0.0773\n",
      "Time taken for 1 epoch 436.8631088733673 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0540\n",
      "Epoch 20 Batch 100 Loss 0.0756\n",
      "Epoch 20 Batch 200 Loss 0.0400\n",
      "Epoch 20 Batch 300 Loss 0.1066\n",
      "Epoch 20 Batch 400 Loss 0.0673\n",
      "Epoch 20 Batch 500 Loss 0.1059\n",
      "Epoch 20 Batch 600 Loss 0.0745\n",
      "Epoch 20 Batch 700 Loss 0.1181\n",
      "Epoch 20 Batch 800 Loss 0.1009\n",
      "Epoch 20 Batch 900 Loss 0.1075\n",
      "Epoch 20 Batch 1000 Loss 0.0868\n",
      "Epoch 20 Batch 1100 Loss 0.1015\n",
      "Epoch 20 Batch 1200 Loss 0.1306\n",
      "Epoch 20 Loss 0.0757\n",
      "Time taken for 1 epoch 438.45683193206787 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0707\n",
      "Epoch 21 Batch 100 Loss 0.0988\n",
      "Epoch 21 Batch 200 Loss 0.0942\n",
      "Epoch 21 Batch 300 Loss 0.0831\n",
      "Epoch 21 Batch 400 Loss 0.0538\n",
      "Epoch 21 Batch 500 Loss 0.0623\n",
      "Epoch 21 Batch 600 Loss 0.0579\n",
      "Epoch 21 Batch 700 Loss 0.1080\n",
      "Epoch 21 Batch 800 Loss 0.0693\n",
      "Epoch 21 Batch 900 Loss 0.0530\n",
      "Epoch 21 Batch 1000 Loss 0.0814\n",
      "Epoch 21 Batch 1100 Loss 0.0556\n",
      "Epoch 21 Batch 1200 Loss 0.1005\n",
      "Epoch 21 Loss 0.0752\n",
      "Time taken for 1 epoch 436.76666808128357 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0536\n",
      "Epoch 22 Batch 100 Loss 0.0250\n",
      "Epoch 22 Batch 200 Loss 0.0766\n",
      "Epoch 22 Batch 300 Loss 0.0478\n",
      "Epoch 22 Batch 400 Loss 0.0898\n",
      "Epoch 22 Batch 500 Loss 0.0861\n",
      "Epoch 22 Batch 600 Loss 0.0660\n",
      "Epoch 22 Batch 700 Loss 0.0712\n",
      "Epoch 22 Batch 800 Loss 0.0753\n",
      "Epoch 22 Batch 900 Loss 0.0620\n",
      "Epoch 22 Batch 1000 Loss 0.0661\n",
      "Epoch 22 Batch 1100 Loss 0.1234\n",
      "Epoch 22 Batch 1200 Loss 0.0747\n",
      "Epoch 22 Loss 0.0745\n",
      "Time taken for 1 epoch 437.9167912006378 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0437\n",
      "Epoch 23 Batch 100 Loss 0.0421\n",
      "Epoch 23 Batch 200 Loss 0.0673\n",
      "Epoch 23 Batch 300 Loss 0.0833\n",
      "Epoch 23 Batch 400 Loss 0.0694\n",
      "Epoch 23 Batch 500 Loss 0.0653\n",
      "Epoch 23 Batch 600 Loss 0.0963\n",
      "Epoch 23 Batch 700 Loss 0.0768\n",
      "Epoch 23 Batch 800 Loss 0.0627\n",
      "Epoch 23 Batch 900 Loss 0.0892\n",
      "Epoch 23 Batch 1000 Loss 0.0732\n",
      "Epoch 23 Batch 1100 Loss 0.1020\n",
      "Epoch 23 Batch 1200 Loss 0.0875\n",
      "Epoch 23 Loss 0.0728\n",
      "Time taken for 1 epoch 439.18765807151794 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0393\n",
      "Epoch 24 Batch 100 Loss 0.0637\n",
      "Epoch 24 Batch 200 Loss 0.0509\n",
      "Epoch 24 Batch 300 Loss 0.0294\n",
      "Epoch 24 Batch 400 Loss 0.0554\n",
      "Epoch 24 Batch 500 Loss 0.0635\n",
      "Epoch 24 Batch 600 Loss 0.0526\n",
      "Epoch 24 Batch 700 Loss 0.0638\n",
      "Epoch 24 Batch 800 Loss 0.1197\n",
      "Epoch 24 Batch 900 Loss 0.0934\n",
      "Epoch 24 Batch 1000 Loss 0.0628\n",
      "Epoch 24 Batch 1100 Loss 0.0862\n",
      "Epoch 24 Batch 1200 Loss 0.1031\n",
      "Epoch 24 Loss 0.0722\n",
      "Time taken for 1 epoch 440.953537940979 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0551\n",
      "Epoch 25 Batch 100 Loss 0.0250\n",
      "Epoch 25 Batch 200 Loss 0.0491\n",
      "Epoch 25 Batch 300 Loss 0.0422\n",
      "Epoch 25 Batch 400 Loss 0.0723\n",
      "Epoch 25 Batch 500 Loss 0.0712\n",
      "Epoch 25 Batch 600 Loss 0.0681\n",
      "Epoch 25 Batch 700 Loss 0.0923\n",
      "Epoch 25 Batch 800 Loss 0.0979\n",
      "Epoch 25 Batch 900 Loss 0.0775\n",
      "Epoch 25 Batch 1000 Loss 0.0771\n",
      "Epoch 25 Batch 1100 Loss 0.0621\n",
      "Epoch 25 Batch 1200 Loss 0.0721\n",
      "Epoch 25 Loss 0.0722\n",
      "Time taken for 1 epoch 439.44482493400574 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0453\n",
      "Epoch 26 Batch 100 Loss 0.0614\n",
      "Epoch 26 Batch 200 Loss 0.0426\n",
      "Epoch 26 Batch 300 Loss 0.0927\n",
      "Epoch 26 Batch 400 Loss 0.0558\n",
      "Epoch 26 Batch 500 Loss 0.0478\n",
      "Epoch 26 Batch 600 Loss 0.0657\n",
      "Epoch 26 Batch 700 Loss 0.0915\n",
      "Epoch 26 Batch 800 Loss 0.0825\n",
      "Epoch 26 Batch 900 Loss 0.0449\n",
      "Epoch 26 Batch 1000 Loss 0.1092\n",
      "Epoch 26 Batch 1100 Loss 0.0953\n",
      "Epoch 26 Batch 1200 Loss 0.0855\n",
      "Epoch 26 Loss 0.0717\n",
      "Time taken for 1 epoch 441.71490597724915 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0729\n",
      "Epoch 27 Batch 100 Loss 0.0328\n",
      "Epoch 27 Batch 200 Loss 0.0561\n",
      "Epoch 27 Batch 300 Loss 0.0811\n",
      "Epoch 27 Batch 400 Loss 0.0575\n",
      "Epoch 27 Batch 500 Loss 0.0691\n",
      "Epoch 27 Batch 600 Loss 0.0564\n",
      "Epoch 27 Batch 700 Loss 0.0676\n",
      "Epoch 27 Batch 800 Loss 0.0951\n",
      "Epoch 27 Batch 900 Loss 0.0754\n",
      "Epoch 27 Batch 1000 Loss 0.0954\n",
      "Epoch 27 Batch 1100 Loss 0.0725\n",
      "Epoch 27 Batch 1200 Loss 0.0845\n",
      "Epoch 27 Loss 0.0702\n",
      "Time taken for 1 epoch 439.28625416755676 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0523\n",
      "Epoch 28 Batch 100 Loss 0.0774\n",
      "Epoch 28 Batch 200 Loss 0.0445\n",
      "Epoch 28 Batch 300 Loss 0.0937\n",
      "Epoch 28 Batch 400 Loss 0.0767\n",
      "Epoch 28 Batch 500 Loss 0.0798\n",
      "Epoch 28 Batch 600 Loss 0.0604\n",
      "Epoch 28 Batch 700 Loss 0.0666\n",
      "Epoch 28 Batch 800 Loss 0.0647\n",
      "Epoch 28 Batch 900 Loss 0.0739\n",
      "Epoch 28 Batch 1000 Loss 0.1156\n",
      "Epoch 28 Batch 1100 Loss 0.0917\n",
      "Epoch 28 Batch 1200 Loss 0.0829\n",
      "Epoch 28 Loss 0.0701\n",
      "Time taken for 1 epoch 441.4612638950348 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0698\n",
      "Epoch 29 Batch 100 Loss 0.0452\n",
      "Epoch 29 Batch 200 Loss 0.0513\n",
      "Epoch 29 Batch 300 Loss 0.0702\n",
      "Epoch 29 Batch 400 Loss 0.0479\n",
      "Epoch 29 Batch 500 Loss 0.0383\n",
      "Epoch 29 Batch 600 Loss 0.0589\n",
      "Epoch 29 Batch 700 Loss 0.0696\n",
      "Epoch 29 Batch 800 Loss 0.0643\n",
      "Epoch 29 Batch 900 Loss 0.0729\n",
      "Epoch 29 Batch 1000 Loss 0.1182\n",
      "Epoch 29 Batch 1100 Loss 0.0944\n",
      "Epoch 29 Batch 1200 Loss 0.1098\n",
      "Epoch 29 Loss 0.0692\n",
      "Time taken for 1 epoch 439.66993498802185 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0776\n",
      "Epoch 30 Batch 100 Loss 0.0597\n",
      "Epoch 30 Batch 200 Loss 0.0781\n",
      "Epoch 30 Batch 300 Loss 0.0968\n",
      "Epoch 30 Batch 400 Loss 0.0860\n",
      "Epoch 30 Batch 500 Loss 0.0558\n",
      "Epoch 30 Batch 600 Loss 0.0715\n",
      "Epoch 30 Batch 700 Loss 0.0627\n",
      "Epoch 30 Batch 800 Loss 0.0586\n",
      "Epoch 30 Batch 900 Loss 0.0710\n",
      "Epoch 30 Batch 1000 Loss 0.0265\n",
      "Epoch 30 Batch 1100 Loss 0.0909\n",
      "Epoch 30 Batch 1200 Loss 0.0949\n",
      "Epoch 30 Loss 0.0683\n",
      "Time taken for 1 epoch 440.25412487983704 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "* And store the *attention weights for every time step*.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1608145599781,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "a5bf709a-7e66-4fd8-aca9-777497144965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x28a80e7d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 985,
     "status": "ok",
     "timestamp": 1619808753710,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "WrAM0FDomq3E",
    "outputId": "d366d6cc-cc03-4e35-a65a-9d06dcbfff36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ce site utilise google . <end>\n",
      "Predicted translation: this guy make a word . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Ce site utilise Google.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1619808761495,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "5bhFfwcIMX5i",
    "outputId": "f89a2d14-a72b-4477-9d76-36837baaefff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> cette est l pour vous . <end>\n",
      "Predicted translation: this is for you . <end> \n"
     ]
    }
   ],
   "source": [
    "translate('Cette est là pour vous.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1619808768959,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "zSx2iM36EZQZ",
    "outputId": "42bc96b4-37c0-439f-fff9-224fdc58c527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ancien tudiant de l'universit . <end>\n",
      "Predicted translation: the man kiss me open a stone . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u\"ancien étudiant de l'université.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3LLCx3ZE0Ls",
    "outputId": "b64aa087-8232-474e-e3c7-98c186081845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> des d marches pour vous inscrire . <end>\n",
      "Predicted translation: sorry costs the effort . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'des démarches pour vous inscrire.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1619808777901,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "DUQVLVqUE1YW",
    "outputId": "6d768ecc-e145-4a4a-b313-0986c44bc1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> pr parer votre professionnelle . <end>\n",
      "Predicted translation: go your friend . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Préparer votre professionnelle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1619808783771,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -180
    },
    "id": "f09_hUFx9EJh",
    "outputId": "73980799-f0f9-4ff5-835c-ced55d9a4770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> un nouvel lan votre carri re . <end>\n",
      "Predicted translation: a girl is being paid to me . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'un nouvel élan à votre carrière.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7c5p8rmkHQG",
    "outputId": "d4682d71-f778-41f5-e4a9-2e1235976123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ou d velopper vos comp tences . <end>\n",
      "Predicted translation: hand over your breathing . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'ou développer vos compétences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdXES85KkTVS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
