{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93224d98",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "#### На вебинаре мы говорили что долгое время CNN и RNN архитектуры были конурируещими выяснить какая архитектура больше подходит для задачи сантимент анализа на данных с вебинара\n",
    "1. построить свёрточные архитектуры\n",
    "2. построить различные архитектуры с RNN\n",
    "3. построить совместные архитектуры CNN -> RNN  и (RNN -> CNN)\n",
    "4. сдлать выводы что получилось лучше\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddf7d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623e6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f791566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_val = pd.read_csv(\"data/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7461d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e203ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1834999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D, SimpleRNN, LSTM, GRU, Masking\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ffffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [stemmer.stem(word) for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af6a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef03ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus_train = df_train['text'].values\n",
    "text_corpus_valid = df_val['text'].values\n",
    "text_corpus_test = df_test['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a7da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=None, \n",
    "                     filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                     lower = False, split = ' ')\n",
    "tokenizer.fit_on_texts(text_corpus_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(text_corpus_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(text_corpus_valid)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_corpus_test)\n",
    "\n",
    "word_count = len(tokenizer.index_word) + 1\n",
    "training_length = max([len(i.split()) for i in text_corpus_train])\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=training_length)\n",
    "X_valid = pad_sequences(sequences_val, maxlen=training_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1693c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31bb8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "437dcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, LSTM, Dense, concatenate, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_cnn_model(input_length, vocab_size):\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "    conv_layer = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
    "    maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "    dropout_layer = Dropout(0.5)(maxpool_layer)  # Добавляем Dropout\n",
    "    global_maxpool = GlobalMaxPooling1D()(dropout_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(global_maxpool)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_rnn_model(input_length, vocab_size):\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "    lstm_layer = LSTM(128, dropout=0.5, recurrent_dropout=0.5)(embedding)  # Добавляем Dropout\n",
    "    output_layer = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_cnn_rnn_model(input_length, vocab_size):\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "    conv_layer = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding)\n",
    "    maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
    "    lstm_layer = LSTM(128, dropout=0.5, recurrent_dropout=0.5)(maxpool_layer)  # Добавляем Dropout\n",
    "    output_layer = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_rnn_cnn_model(input_length, vocab_size):\n",
    "    inputs = Input(shape=(input_length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "    lstm_layer = LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)(embedding)  # Добавляем Dropout\n",
    "    conv_layer = Conv1D(filters=128, kernel_size=3, activation='relu')(lstm_layer)\n",
    "    global_maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')(global_maxpool)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46f66979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247859\n",
      "247838\n"
     ]
    }
   ],
   "source": [
    "# Проверим значения индексов в X_train и X_valid\n",
    "print(np.max(X_train))\n",
    "print(np.max(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3864f2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 247860\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 17s 52ms/step - loss: 0.5862 - accuracy: 0.6796 - val_loss: 0.5464 - val_accuracy: 0.7247\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 16s 50ms/step - loss: 0.3681 - accuracy: 0.8404 - val_loss: 0.5560 - val_accuracy: 0.7147\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 85s 263ms/step - loss: 0.5495 - accuracy: 0.7078 - val_loss: 0.4924 - val_accuracy: 0.7536\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 87s 272ms/step - loss: 0.3411 - accuracy: 0.8536 - val_loss: 0.5149 - val_accuracy: 0.7505\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 49s 152ms/step - loss: 0.5812 - accuracy: 0.6835 - val_loss: 0.5284 - val_accuracy: 0.7326\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 50s 156ms/step - loss: 0.3422 - accuracy: 0.8515 - val_loss: 0.5914 - val_accuracy: 0.7167\n",
      "Epoch 1/10\n",
      "319/319 [==============================] - 60s 186ms/step - loss: 0.5591 - accuracy: 0.6975 - val_loss: 0.4871 - val_accuracy: 0.7589\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 59s 185ms/step - loss: 0.3332 - accuracy: 0.8570 - val_loss: 0.5170 - val_accuracy: 0.7525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x299142510>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Установим vocab_size\n",
    "vocab_size = np.max(np.concatenate([X_train, X_valid])) + 1\n",
    "\n",
    "print(\"Vocab Size:\", vocab_size)\n",
    "\n",
    "# Предположим, что maxlen - это максимальная длина последовательности\n",
    "maxlen = X_train.shape[1]\n",
    "\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "# код для создания модели\n",
    "cnn_model = build_cnn_model(maxlen, vocab_size)\n",
    "cnn_model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "rnn_model = build_rnn_model(maxlen, vocab_size)\n",
    "rnn_model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "cnn_rnn_model = build_cnn_rnn_model(maxlen, vocab_size)\n",
    "cnn_rnn_model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "rnn_cnn_model = build_rnn_cnn_model(maxlen, vocab_size)\n",
    "rnn_cnn_model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc80c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 6ms/step - loss: 0.5847 - accuracy: 0.6971\n",
      "\n",
      " cnn_model\n",
      "Test score cnn_model: 0.5846816301345825\n",
      "Test accuracy cnn_model: 0.6970859169960022\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.5519 - accuracy: 0.7408\n",
      "\n",
      " rnn_model\n",
      "Test score rnn_model: 0.5518957376480103\n",
      "Test accuracy rnn_model: 0.7407750487327576\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.6403 - accuracy: 0.6981\n",
      "\n",
      " cnn_rnn_model\n",
      "Test score cnn_rnn_model: 0.6403089165687561\n",
      "Test accuracy cnn_rnn_model: 0.6980558037757874\n",
      "45/45 [==============================] - 2s 37ms/step - loss: 0.5450 - accuracy: 0.7404\n",
      "\n",
      " rnn_cnn_model\n",
      "Test score rnn_cnn_model: 0.5449795722961426\n",
      "Test accuracy rnn_cnn_model: 0.7403782606124878\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n cnn_model')\n",
    "print('Test score cnn_model:', score[0])\n",
    "print('Test accuracy cnn_model:', score[1])\n",
    "\n",
    "score = rnn_model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n rnn_model')\n",
    "print('Test score rnn_model:', score[0])\n",
    "print('Test accuracy rnn_model:', score[1])\n",
    "\n",
    "score = cnn_rnn_model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n cnn_rnn_model')\n",
    "print('Test score cnn_rnn_model:', score[0])\n",
    "print('Test accuracy cnn_rnn_model:', score[1])\n",
    "\n",
    "score = rnn_cnn_model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n rnn_cnn_model')\n",
    "print('Test score rnn_cnn_model:', score[0])\n",
    "print('Test accuracy rnn_cnn_model:', score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02254dac",
   "metadata": {},
   "source": [
    "### ВЫВОДЫ:\n",
    "- Сравнивая значения Test accuracy и Test loss, можно сказать, что модели rnn_model и rnn_cnn_model показывают лучшие результаты по сравнению с cnn_model и cnn_rnn_model.\n",
    "\n",
    "- Таким образом, модели, которые включают слои рекуррентной нейронной сети (RNN), rnn_model и rnn_cnn_model, имеют лучшую обобщающую способность на тестовых данных для задачи сентимент анализа по сравнению с моделями cnn_model и cnn_rnn_model.\n",
    "\n",
    "- Модель cnn_rnn_model содержит сверточный слой (Conv1D), который применяется перед слоем LSTM. Сверточные слои обладают свойством извлекать локальные признаки, в то время как рекуррентные слои (LSTM) способны улавливать долгосрочные зависимости в последовательностях. Уловить долгосрочные зависимости после свертки может быть достаточно сложно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
